{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee08887",
   "metadata": {},
   "source": [
    "\n",
    "# Enhancing SQL Server Searches with Elasticsearch and Python\n",
    "\n",
    "As a seasoned SQL developer and tech enthusiast, I often encounter scenarios where traditional SQL Server searches struggle to meet performance and flexibility requirements. One such challenge is efficiently searching a large person database with multiple parameters and fuzzy logic. In this blog post, I'll explore how integrating Elasticsearch with SQL Server using Python can offer a robust solution to this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e86104e",
   "metadata": {},
   "source": [
    "\n",
    "## Business Problem\n",
    "\n",
    "Imagine you're tasked with developing a search functionality for a vast person database. Users need the ability to search using various parameters such as:\n",
    "- First Name\n",
    "- Last Name\n",
    "- Preferred Name\n",
    "- City\n",
    "- State\n",
    "- ZipCode\n",
    "- DOB\n",
    "- Email\n",
    "\n",
    "The complexity increases as searches can occur on any combination of these fields, and there's a need to implement fuzzy logic to handle typos and partial matches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22592a65",
   "metadata": {},
   "source": [
    "\n",
    "## Technical Challenges with SQL Server\n",
    "\n",
    "### Inefficient Indexing\n",
    "\n",
    "Creating indexes on every possible combination of the searchable columns isn't practical. The number of required indexes grows exponentially with each additional column, leading to:\n",
    "\n",
    "- Increased storage requirements\n",
    "- Slower write operations due to index maintenance\n",
    "- Diminishing returns on query performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec68c2",
   "metadata": {},
   "source": [
    "\n",
    "### Implementing Fuzzy Logic\n",
    "\n",
    "While SQL Server offers some capabilities like the LIKE operator or full-text search, these methods are limited:\n",
    "\n",
    "- **Performance Issues**: Fuzzy searches can be slow on large datasets.\n",
    "- **Complexity**: Implementing advanced fuzzy matching (e.g., handling typos, synonyms) requires intricate queries and functions that can degrade performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92afb532",
   "metadata": {},
   "source": [
    "\n",
    "## High-Level Solution: Integrating Elasticsearch\n",
    "\n",
    "To overcome these challenges, I recommend integrating Elasticsearch into your search architecture. Here's why:\n",
    "\n",
    "- **Optimized for Search**: Elasticsearch is designed for lightning-fast full-text searches across multiple fields.\n",
    "- **Fuzzy Matching**: It natively supports fuzzy logic, allowing for more flexible and user-friendly search experiences.\n",
    "- **Scalability**: Built to handle large volumes of data and queries efficiently.\n",
    "- **Existing Infrastructure**: Many production environments already use Elasticsearch for logging, making integration smoother.\n",
    "- **Cost-Effective**: The basic version is free and sufficient for many use cases. For advanced security features, consider the paid tiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0693d7",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation Overview with Python\n",
    "\n",
    "While Elasticsearch can be integrated using various programming languages, I'll demonstrate how to use Python for this purpose. Python's rich ecosystem and simplicity make it an excellent choice for interfacing with Elasticsearch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb3f64",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Preparing Source Data\n",
    "\n",
    "- Create a Sample Database: Use SQL Server to set up your person database, including all relevant fields.\n",
    "- Populate Data: Insert sample records in sql server to simulate a realistic dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f273a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyodbc\n",
    "%pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pyodbc\n",
    "import random\n",
    "import datetime\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import time\n",
    "from colorama import Fore, Style\n",
    "\n",
    "# Define server name as a global variable so it can be set once\n",
    "server = 'localhost'\n",
    "db_name = 'PersonSearchDB'\n",
    "\n",
    "def create_database_if_not_exists(db_name):\n",
    "    # Connection to 'master' for creating the database\n",
    "    master_conn_str = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE=master;Trusted_Connection=yes;'\n",
    "    master_conn = pyodbc.connect(master_conn_str)\n",
    "\n",
    "    # Set autocommit to True to disable transactions\n",
    "    master_conn.autocommit = True\n",
    "\n",
    "    master_cursor = master_conn.cursor()\n",
    "\n",
    "    # Check if the database exists, if not, create it\n",
    "    create_db_query = f\"IF DB_ID('{db_name}') IS NULL CREATE DATABASE {db_name};\"\n",
    "    master_cursor.execute(create_db_query)\n",
    "\n",
    "    # Close master connection\n",
    "    master_cursor.close()\n",
    "    master_conn.close()\n",
    "\n",
    "# Function to return SQL Server connection\n",
    "def get_sql_connection(db_name):\n",
    "    \"\"\"\n",
    "    Returns a SQL Server connection object for the given database name.\n",
    "    If the database does not exist, the function connects to 'master' first to create it.\n",
    "    \n",
    "    Parameters:\n",
    "    db_name (str): The name of the database to connect to.\n",
    "\n",
    "    Returns:\n",
    "    pyodbc.Connection: A connection object to the specified database.\n",
    "    \"\"\"    \n",
    "\n",
    "    # Connection to the newly created or existing database\n",
    "    conn_str = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={db_name};Trusted_Connection=yes;'\n",
    "    return pyodbc.connect(conn_str)\n",
    "\n",
    "def execute_SQL_Query(db_name, query, params=None):\n",
    "    \"\"\"\n",
    "    Executes a SQL query on the given database. Uses the connection obtained from get_sql_connection.\n",
    "    \n",
    "    Parameters:\n",
    "    db_name (str): The name of the database.\n",
    "    query (str): The SQL query to execute.\n",
    "    params (tuple): Parameters to pass to the query (optional).\n",
    "    \n",
    "    Returns:\n",
    "    list: The result of the query if it's a SELECT query, otherwise None.\n",
    "    \"\"\"\n",
    "    conn = get_sql_connection(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    if params:\n",
    "        cursor.execute(query, params)\n",
    "    else:\n",
    "        cursor.execute(query)\n",
    "    \n",
    "    if query.strip().upper().startswith(\"SELECT\"):\n",
    "        result = cursor.fetchall()  # Fetch all results if it's a SELECT query\n",
    "    else:\n",
    "        conn.commit()\n",
    "        result = None\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def setup_database_and_bulk_insert_data(db_name, record_count=1000, batch_size=100):\n",
    "    # List of Indian first names, last names, cities, and states\n",
    "    first_names = [\n",
    "        'Rahul', 'Anjali', 'Amit', 'Pooja', 'Rajesh', 'Sneha', 'Vikram', 'Neha', 'Suresh', 'Sunita',\n",
    "        'Arjun', 'Kiran', 'Ravi', 'Priya', 'Nikhil', 'Meera', 'Kunal', 'Rina', 'Aakash', 'Divya',\n",
    "        'Sanjay', 'Anita', 'Deepak', 'Kavita', 'Manish', 'Shweta', 'Rohit', 'Preeti', 'Vijay', 'Swati',\n",
    "        'Ajay', 'Nisha', 'Gaurav', 'Shalini', 'Alok', 'Tanvi', 'Varun', 'Shruti', 'Vivek', 'Rashmi'\n",
    "    ]\n",
    "    last_names = [\n",
    "        'Sharma', 'Patel', 'Gupta', 'Mehta', 'Jain', 'Agarwal', 'Reddy', 'Singh', 'Kumar', 'Verma',\n",
    "        'Chopra', 'Desai', 'Iyer', 'Joshi', 'Kapoor', 'Malhotra', 'Nair', 'Pandey', 'Rao', 'Saxena',\n",
    "        'Bose', 'Chatterjee', 'Das', 'Mukherjee', 'Banerjee', 'Bhat', 'Pillai', 'Menon', 'Choudhury', 'Trivedi',\n",
    "        'Shah', 'Parekh', 'Chauhan', 'Patil', 'Dutta', 'Nayar', 'Kulkarni', 'Bhattacharya', 'Hegde', 'Sinha'\n",
    "    ]\n",
    "    cities = [\n",
    "        'Mumbai', 'Delhi', 'Bangalore', 'Chennai', 'Hyderabad', 'Ahmedabad', 'Kolkata', 'Pune', 'Jaipur', 'Lucknow',\n",
    "        'Surat', 'Kanpur', 'Nagpur', 'Indore', 'Thane', 'Bhopal', 'Visakhapatnam', 'Patna', 'Vadodara', 'Ghaziabad'\n",
    "    ]\n",
    "    states = [\n",
    "        'MH', 'DL', 'KA', 'TN', 'TS', 'GJ', 'WB', 'MH', 'RJ', 'UP',\n",
    "        'MP', 'AP', 'BR', 'HR', 'PB', 'KL', 'OR', 'AS', 'JK', 'CH'\n",
    "    ]\n",
    "\n",
    "    def random_name():\n",
    "        return random.choice(first_names), random.choice(last_names)\n",
    "\n",
    "    def random_email(first_name, last_name):\n",
    "        return f\"{first_name.lower()}.{last_name.lower()}@randommail.com\"\n",
    "\n",
    "    def random_DOB():\n",
    "        start_date = datetime.date(1950, 1, 1)\n",
    "        end_date = datetime.date(2005, 12, 31)\n",
    "        time_between_dates = end_date - start_date\n",
    "        days_between_dates = time_between_dates.days\n",
    "        random_number_of_days = random.randrange(days_between_dates)\n",
    "        random_date = start_date + datetime.timedelta(days=random_number_of_days)\n",
    "        return random_date\n",
    "\n",
    "    def random_zipcode():\n",
    "        return str(random.randint(100000, 999999))  # Indian zip codes are 6 digits\n",
    "\n",
    "    # Create Persons table using execute_SQL_Query\n",
    "    create_table_query = '''\n",
    "    IF OBJECT_ID('Persons', 'U') IS NOT NULL DROP TABLE Persons;\n",
    "    CREATE TABLE Persons (\n",
    "        FirstName NVARCHAR(50),\n",
    "        LastName NVARCHAR(50),\n",
    "        PreferredName NVARCHAR(50),\n",
    "        City NVARCHAR(50),\n",
    "        State NVARCHAR(50),\n",
    "        ZipCode NVARCHAR(10),\n",
    "        DOB DATE,\n",
    "        Email NVARCHAR(100)\n",
    "    );\n",
    "    '''\n",
    "    execute_SQL_Query(db_name, create_table_query)\n",
    "\n",
    "    # Insert records in batches\n",
    "    for batch_start in range(0, record_count, batch_size):\n",
    "        values = []\n",
    "        for _ in range(batch_size):\n",
    "            first_name, last_name = random_name()\n",
    "            preferred_name = first_name  # Assume preferred name is the first name\n",
    "            city = random.choice(cities)\n",
    "            state = random.choice(states)\n",
    "            zipcode = random_zipcode()\n",
    "            dob = random_DOB()\n",
    "            dob_str = dob.strftime('%Y-%m-%d')\n",
    "            email = random_email(first_name, last_name)\n",
    "            values.append(f\"SELECT '{first_name}', '{last_name}', '{preferred_name}', '{city}', '{state}', '{zipcode}', '{dob_str}', '{email}'\")\n",
    "\n",
    "        # Create bulk insert query using INSERT INTO ... SELECT\n",
    "        insert_query = '''\n",
    "        INSERT INTO Persons (FirstName, LastName, PreferredName, City, State, ZipCode, DOB, Email)\n",
    "        ''' + \" UNION ALL \".join(values)\n",
    "\n",
    "        execute_SQL_Query(db_name, insert_query)\n",
    "        print(f\"Inserted batch starting at record {batch_start}\")\n",
    "        \n",
    "    #Master Database Creation\n",
    "create_database_if_not_exists(db_name)\n",
    "\n",
    "#Setup Database and Bulk Insert Data  \n",
    "#I am inserting million records to demo perfromance gains. Feel free to adjust for testing purpose.\n",
    "setup_database_and_bulk_insert_data(db_name, record_count=1000000, batch_size=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b7350",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Setting Up Elasticsearch\n",
    "\n",
    "- Download and Install: Obtain Elasticsearch from the official website and follow the installation guide.\n",
    "- Configuration: Adjust settings as needed, such as cluster name and network configurations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eb9c1a",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Creating and Populating Elasticsearch Index\n",
    "\n",
    "Define Mappings: Specify how each field should be indexed and analyzed, particularly those requiring fuzzy search capabilities.\n",
    "\n",
    "\n",
    "#### Data Extraction and Bulk Insertion\n",
    "\n",
    "Extract data from SQL Server and load it into Elasticsearch.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ded0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import urllib3\n",
    "\n",
    "# Suppress warnings about insecure connections (optional)\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "   \n",
    "# Replace with your actual password\n",
    "elastic_password = \"IMoWOv8DHTMNnQod37NS\"\n",
    "\n",
    "# Initialize the Elasticsearch client with SSL and authentication\n",
    "# make sure elastic search is running on port 9200, \n",
    "# Use the following command to start elastic search\n",
    "# .\\elasticsearch-8.11.1\\bin\\elasticsearch.bat\n",
    "\n",
    "\n",
    "es = Elasticsearch(\n",
    "    [\"https://localhost:9200\"],\n",
    "    ca_certs=False,          # Disable SSL certificate verification\n",
    "    verify_certs=False,      # Disable SSL cert verification (use with caution)\n",
    "    basic_auth=(\"elastic\", elastic_password),\n",
    ")\n",
    "\n",
    "def index_data_to_elasticsearch(db_name, index_name, batch_size=10000):\n",
    "    \n",
    "    # Delete the existing index if it exists\n",
    "    if es.indices.exists(index=index_name):\n",
    "        es.indices.delete(index=index_name)\n",
    "        print(f\"Deleted existing index: {index_name}\")\n",
    "\n",
    "    # Create a new index with mappings\n",
    "    index_mappings = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"FirstName\": {\"type\": \"text\"},\n",
    "                \"LastName\": {\"type\": \"text\"},\n",
    "                \"PreferredName\": {\"type\": \"text\"},\n",
    "                \"City\": {\"type\": \"text\"},\n",
    "                \"State\": {\"type\": \"keyword\"},\n",
    "                \"ZipCode\": {\"type\": \"keyword\"},\n",
    "                \"DOB\": {\"type\": \"date\"},\n",
    "                \"Email\": {\"type\": \"keyword\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    es.indices.create(index=index_name, body=index_mappings)\n",
    "    print(f\"Created new index: {index_name}\")\n",
    "\n",
    "    # Get total number of records\n",
    "    total_records_query = \"SELECT COUNT(*) FROM Persons\"\n",
    "    total_records_result = execute_SQL_Query(db_name, total_records_query)\n",
    "    total_records = total_records_result[0][0]\n",
    "\n",
    "    offset = 0\n",
    "    while offset < total_records:\n",
    "        query = f'''\n",
    "        SELECT FirstName, LastName, PreferredName, City, State, ZipCode, DOB, Email\n",
    "        FROM Persons\n",
    "        ORDER BY FirstName\n",
    "        OFFSET {offset} ROWS FETCH NEXT {batch_size} ROWS ONLY\n",
    "        '''\n",
    "        rows = execute_SQL_Query(db_name, query)\n",
    "        actions = []\n",
    "        for row in rows:\n",
    "            print(f\"\\n\\nrow: {row}\")\n",
    "            doc = {\n",
    "                \"_index\": index_name,\n",
    "                \"_source\": {\n",
    "                    \"FirstName\": row[0],\n",
    "                    \"LastName\": row[1],\n",
    "                    \"PreferredName\": row[2],\n",
    "                    \"City\": row[3],\n",
    "                    \"State\": row[4],\n",
    "                    \"ZipCode\": row[5],\n",
    "                    \"DOB\": row[6].strftime('%Y-%m-%d') if row[6] else None,\n",
    "                    \"Email\": row[7]\n",
    "                }\n",
    "            }\n",
    "            actions.append(doc)\n",
    "            \n",
    "        try:\n",
    "            helpers.bulk(es, actions)\n",
    "        except helpers.BulkIndexError as e:\n",
    "            print(f\"Bulk indexing error: {e}\")\n",
    "            for error in e.errors:\n",
    "                print(error)\n",
    "        \n",
    "        offset += batch_size\n",
    "        print(f\"Indexed {offset}/{total_records} records\")\n",
    "\n",
    "# Example usage: setting up the database and bulk inserting data\n",
    "# setup_database_and_bulk_insert_data(db_name, record_count=1000, batch_size=100)\n",
    "index_data_to_elasticsearch('PersonSearchDB', 'person_index', batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382efb9",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Implementing Search Queries\n",
    "\n",
    "- Basic Searches: Perform searches on single or multiple fields.\n",
    "- Fuzzy Searches: Utilize Elasticsearch's fuzzy query capabilities to handle misspellings and partial matches.\n",
    "- Boolean search:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e2921",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ExecuteElasticSearch(search_query):\n",
    "    batch_size=10\n",
    "    # Execute the search\n",
    "    response = es.search(index=\"people_index\", body=search_query, size=batch_size)\n",
    "    \n",
    "    # Process the results\n",
    "    print(f\"Found {response['hits']['total']['value']} documents:\")\n",
    "    for hit in response['hits']['hits']:\n",
    "        print(f\"\\n\\n\\n{hit['_source']}\")\n",
    "\n",
    "\n",
    "        \n",
    "def first_name_search(column_name, first_name):\n",
    "    search_query = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                column_name: first_name\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    ExecuteElasticSearch(search_query)\n",
    "\n",
    "def multi_field_wildcard_search(first_name, last_name, birth_year):    \n",
    "    search_query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\"wildcard\": {\"FirstName\": f\"*{first_name}*\"}},\n",
    "                    {\"wildcard\": {\"LastName\": f\"*{last_name}*\"}},\n",
    "                    {\n",
    "                        \"range\": {\n",
    "                            \"BirthDate\": {\n",
    "                                \"gte\": f\"{birth_year}-01-01\",\n",
    "                                \"lte\": f\"{birth_year}-12-31\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    ExecuteElasticSearch(search_query)\n",
    "\n",
    "def fuzzy_logic_search(first_name):\n",
    "    search_query = {\n",
    "        \"query\": {\n",
    "            \"fuzzy\": {\n",
    "                \"FirstName\": {\n",
    "                    \"value\": first_name,\n",
    "                    \"fuzziness\": 2\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }    \n",
    "    ExecuteElasticSearch(search_query)\n",
    "    \n",
    "def boolean_logic_search(first_name, last_name, birth_year):\n",
    "    search_query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\"wildcard\": {\"FirstName\": f\"*{first_name}*\"}},\n",
    "                    {\"wildcard\": {\"LastName\": f\"*{last_name}*\"}},\n",
    "                    {\n",
    "                        \"range\": {\n",
    "                            \"BirthDate\": {\n",
    "                                \"gte\": f\"{birth_year}-01-01\",\n",
    "                                \"lte\": f\"{birth_year}-12-31\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"minimum_should_match\": 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    ExecuteElasticSearch(search_query)\n",
    "\n",
    "# Example usage of search functions\n",
    "first_name_search(\"FirstName\", \"Rahul\")\n",
    "\n",
    "#Multi Field Wildcard Search\n",
    "multi_field_wildcard_search(\"Rahul\", \"Sharma\", 1990)\n",
    "\n",
    "#Fuzzy Logic Search\n",
    "fuzzy_logic_search(\"Rahul\")\n",
    "\n",
    "#Boolean Logic Search\n",
    "boolean_logic_search(\"Rahul\", \"Sharma\", 1990)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e3f0ae",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Performance Comparison\n",
    "\n",
    "To illustrate the benefits:\n",
    "\n",
    "- **Unindexed SQL Server Searches**: Execute searches without indexes to highlight performance issues.\n",
    "- **Elasticsearch Searches**: Run equivalent searches and compare response times and accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16e68102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mAverage SQL execution time: 0.075508 seconds\u001b[0m\n",
      "\u001b[92mAverage Elasticsearch execution time: 0.002892 seconds\u001b[0m\n",
      "\u001b[93mElasticsearch performance gain: 96.17%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compare_performance(first_name, last_name, preferred_name, iterations=1000):\n",
    "    # SQL query\n",
    "    sql_query = '''\n",
    "    SELECT FirstName, LastName, PreferredName\n",
    "    FROM Persons\n",
    "    WHERE FirstName = ? AND LastName = ? AND PreferredName LIKE ?\n",
    "    '''\n",
    "\n",
    "    # Warm-up run for SQL Server\n",
    "    execute_SQL_Query(db_name, sql_query, (first_name, last_name, preferred_name))\n",
    "\n",
    "    # Record execution times for SQL Server\n",
    "    sql_execution_times = []\n",
    "    for _ in range(iterations):\n",
    "        start_time = time.perf_counter()\n",
    "        execute_SQL_Query(db_name, sql_query, (first_name, last_name, preferred_name))\n",
    "        end_time = time.perf_counter()\n",
    "        sql_execution_times.append(end_time - start_time)\n",
    "\n",
    "    # Calculate average execution time for SQL Server\n",
    "    average_time_sql = sum(sql_execution_times) / iterations\n",
    "\n",
    "    # Elasticsearch query\n",
    "    es_query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\"match\": {\"FirstName\": first_name}},\n",
    "                    {\"match\": {\"LastName\": last_name}},\n",
    "                    {\"wildcard\": {\"PreferredName\": preferred_name}}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Warm-up run for Elasticsearch\n",
    "    es.search(index=\"person_index\", body=es_query)\n",
    "\n",
    "    # Record execution times for Elasticsearch\n",
    "    es_execution_times = []\n",
    "    for _ in range(iterations):\n",
    "        start_time = time.perf_counter()\n",
    "        es.search(index=\"person_index\", body=es_query)\n",
    "        end_time = time.perf_counter()\n",
    "        es_execution_times.append(end_time - start_time)\n",
    "\n",
    "    # Calculate average execution time for Elasticsearch\n",
    "    average_time_es = sum(es_execution_times) / iterations\n",
    "\n",
    "    print(f\"\\033[91mAverage SQL execution time: {average_time_sql:.6f} seconds\\033[0m\")\n",
    "    print(f\"\\033[92mAverage Elasticsearch execution time: {average_time_es:.6f} seconds\\033[0m\")\n",
    "    performance_gain = (average_time_sql - average_time_es) / average_time_sql * 100\n",
    "    print(f\"\\033[93mElasticsearch performance gain: {performance_gain:.2f}%\\033[0m\")\n",
    "    \n",
    "#Performance Comparison\n",
    "compare_performance(\"Rahul\", \"Sharma\", \"Rahul%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e380dba",
   "metadata": {},
   "source": [
    "\n",
    "## Addressing Production Concerns\n",
    "\n",
    "### Why Not Implement Python Code in SQL Server?\n",
    "\n",
    "While SQL Server supports Python via Machine Learning Services, enabling this feature in production can pose security risks.\n",
    "\n",
    "- **Security Risk**: Activating external scripting (`sp_configure 'external scripts enabled', 1`) can expose your server to potential vulnerabilities.\n",
    "- **Resource Overhead**: Running Python scripts within SQL Server can consume significant resources, affecting database performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2458d57",
   "metadata": {},
   "source": [
    "\n",
    "### Hardware Requirements for Elasticsearch\n",
    "\n",
    "- **Modest Initial Setup**: Elasticsearch can run on standard hardware for small to medium datasets.\n",
    "- **Scalability**: For larger datasets and higher query volumes, plan for additional resources or consider a cluster setup.\n",
    "- **Monitoring**: Keep an eye on CPU, memory, and disk I/O to ensure optimal performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e35eb81",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Integrating Elasticsearch with SQL Server can significantly enhance search capabilities, especially for large datasets requiring flexible and fuzzy searches. While it introduces additional components to your architecture, the performance gains and improved user experience can be well worth the effort.\n",
    "\n",
    "Remember, handling PII necessitates a careful approach to security. By following best practices and possibly opting for Elasticsearch's paid tiers for advanced security features, you can create a powerful, secure, and efficient search solution.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
